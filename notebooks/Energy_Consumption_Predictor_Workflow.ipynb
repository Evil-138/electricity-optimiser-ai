{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08f983b",
   "metadata": {},
   "source": [
    "# Energy Consumption Predictor - Complete Workflow\n",
    "\n",
    "This notebook demonstrates the complete workflow of the Energy Consumption Predictor project, from data exploration to model training and prediction.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Data Loading and Exploration](#data-exploration)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Feature Engineering](#feature-engineering)\n",
    "5. [Model Training and Evaluation](#modeling)\n",
    "6. [Predictions and Visualization](#predictions)\n",
    "7. [API Usage Examples](#api-usage)\n",
    "8. [Results Analysis](#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3c2aa",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb29f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_preprocessing import DataPreprocessor\n",
    "from feature_engineering import FeatureEngineer\n",
    "from modeling import EnergyPredictionPipeline\n",
    "from train_pipeline import ModelTrainer\n",
    "from visualization import EnergyVisualizationSuite\n",
    "from utils import load_config, Timer\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c72027",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration {#data-exploration}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "electricity_path = '../data/raw/electricity_consumption.csv'\n",
    "weather_path = '../data/raw/weather_data.csv'\n",
    "\n",
    "# Check if sample data exists, if not generate it\n",
    "if not os.path.exists(electricity_path) or not os.path.exists(weather_path):\n",
    "    print(\"Sample data not found. Generating sample data...\")\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, '../generate_sample_data.py'])\n",
    "\n",
    "# Load electricity consumption data\n",
    "electricity_df = pd.read_csv(electricity_path)\n",
    "electricity_df['timestamp'] = pd.to_datetime(electricity_df['timestamp'])\n",
    "electricity_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Load weather data\n",
    "weather_df = pd.read_csv(weather_path)\n",
    "weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp'])\n",
    "weather_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "print(f\"Electricity data shape: {electricity_df.shape}\")\n",
    "print(f\"Weather data shape: {weather_df.shape}\")\n",
    "print(f\"\\nElectricity data columns: {list(electricity_df.columns)}\")\n",
    "print(f\"Weather data columns: {list(weather_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36db6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"Electricity Consumption Statistics:\")\n",
    "print(electricity_df.describe())\n",
    "\n",
    "print(\"\\nWeather Data Statistics:\")\n",
    "print(weather_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bffae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Electricity consumption over time\n",
    "axes[0, 0].plot(electricity_df.index, electricity_df['kwh'])\n",
    "axes[0, 0].set_title('Electricity Consumption Over Time')\n",
    "axes[0, 0].set_ylabel('kWh')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Temperature over time\n",
    "axes[0, 1].plot(weather_df.index, weather_df['temperature_c'], color='red')\n",
    "axes[0, 1].set_title('Temperature Over Time')\n",
    "axes[0, 1].set_ylabel('Temperature (°C)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Consumption distribution\n",
    "axes[1, 0].hist(electricity_df['kwh'], bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Distribution of Electricity Consumption')\n",
    "axes[1, 0].set_xlabel('kWh')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Temperature vs Consumption scatter\n",
    "merged_temp = pd.merge(electricity_df, weather_df[['temperature_c']], left_index=True, right_index=True, how='inner')\n",
    "axes[1, 1].scatter(merged_temp['temperature_c'], merged_temp['kwh'], alpha=0.6)\n",
    "axes[1, 1].set_title('Temperature vs Electricity Consumption')\n",
    "axes[1, 1].set_xlabel('Temperature (°C)')\n",
    "axes[1, 1].set_ylabel('kWh')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7904d72",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing {#preprocessing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data preprocessor\n",
    "preprocessor = DataPreprocessor(electricity_path, weather_path)\n",
    "\n",
    "# Run complete preprocessing pipeline\n",
    "with Timer(\"Data Preprocessing\"):\n",
    "    processed_data = preprocessor.preprocess_pipeline(\n",
    "        missing_method='interpolate',\n",
    "        frequency='H',\n",
    "        outlier_method='iqr',\n",
    "        remove_outliers=True\n",
    "    )\n",
    "\n",
    "print(f\"Processed data shape: {processed_data.shape}\")\n",
    "print(f\"Date range: {processed_data.index.min()} to {processed_data.index.max()}\")\n",
    "\n",
    "# Display processed data sample\n",
    "print(\"\\nProcessed Data Sample:\")\n",
    "print(processed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aeaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data summary\n",
    "summary = preprocessor.get_data_summary()\n",
    "\n",
    "print(\"Data Summary:\")\n",
    "print(f\"Total records: {summary['total_records']}\")\n",
    "print(f\"Duration: {summary['date_range']['duration_days']} days\")\n",
    "print(f\"Average consumption: {summary['electricity_stats']['mean_kwh']:.2f} kWh\")\n",
    "print(f\"Peak consumption: {summary['electricity_stats']['max_kwh']:.2f} kWh\")\n",
    "print(f\"Min consumption: {summary['electricity_stats']['min_kwh']:.2f} kWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed87e6",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering {#feature-engineering}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer(target_col='kwh')\n",
    "\n",
    "# Run complete feature engineering\n",
    "with Timer(\"Feature Engineering\"):\n",
    "    engineered_data = feature_engineer.engineer_all_features(\n",
    "        processed_data,\n",
    "        lag_hours=[1, 2, 3, 6, 12, 24, 48, 72, 168],\n",
    "        rolling_windows=[6, 12, 24, 48, 168],\n",
    "        rolling_stats=['mean', 'std', 'min', 'max'],\n",
    "        ema_alphas=[0.1, 0.3, 0.5, 0.7]\n",
    "    )\n",
    "\n",
    "print(f\"Engineered data shape: {engineered_data.shape}\")\n",
    "print(f\"Total features created: {len(feature_engineer.get_feature_names())}\")\n",
    "\n",
    "# Display feature groups\n",
    "feature_groups = feature_engineer.get_feature_groups()\n",
    "print(\"\\nFeature Groups:\")\n",
    "for group, features in feature_groups.items():\n",
    "    print(f\"{group}: {len(features)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c428c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some engineered features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Original vs lag features\n",
    "axes[0, 0].plot(engineered_data.index[-168:], engineered_data['kwh'].iloc[-168:], label='Original', alpha=0.8)\n",
    "axes[0, 0].plot(engineered_data.index[-168:], engineered_data['kwh_lag_24h'].iloc[-168:], label='Lag 24h', alpha=0.8)\n",
    "axes[0, 0].set_title('Original vs Lag Features (Last Week)')\n",
    "axes[0, 0].set_ylabel('kWh')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Rolling mean features\n",
    "axes[0, 1].plot(engineered_data.index[-168:], engineered_data['kwh'].iloc[-168:], label='Original', alpha=0.8)\n",
    "axes[0, 1].plot(engineered_data.index[-168:], engineered_data['kwh_rolling_24h_mean'].iloc[-168:], label='Rolling 24h Mean', alpha=0.8)\n",
    "axes[0, 1].set_title('Original vs Rolling Mean')\n",
    "axes[0, 1].set_ylabel('kWh')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hourly patterns\n",
    "hourly_avg = engineered_data.groupby('hour')['kwh'].mean()\n",
    "axes[1, 0].plot(hourly_avg.index, hourly_avg.values, marker='o')\n",
    "axes[1, 0].set_title('Average Consumption by Hour')\n",
    "axes[1, 0].set_xlabel('Hour of Day')\n",
    "axes[1, 0].set_ylabel('Average kWh')\n",
    "axes[1, 0].set_xticks(range(0, 24, 2))\n",
    "\n",
    "# Weekend vs weekday patterns\n",
    "weekend_avg = engineered_data[engineered_data['is_weekend'] == 1]['kwh'].mean()\n",
    "weekday_avg = engineered_data[engineered_data['is_weekend'] == 0]['kwh'].mean()\n",
    "axes[1, 1].bar(['Weekday', 'Weekend'], [weekday_avg, weekend_avg], alpha=0.7)\n",
    "axes[1, 1].set_title('Average Consumption: Weekday vs Weekend')\n",
    "axes[1, 1].set_ylabel('Average kWh')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd7f65",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation {#modeling}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b868db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prediction pipeline\n",
    "pipeline = EnergyPredictionPipeline(target_col='kwh')\n",
    "\n",
    "# Train all models\n",
    "with Timer(\"Model Training\"):\n",
    "    results = pipeline.train_pipeline(engineered_data, test_size=0.2)\n",
    "\n",
    "print(\"\\nModel Training Results:\")\n",
    "print(f\"Models trained: {len(pipeline.models)}\")\n",
    "print(f\"Features used: {len(pipeline.feature_names)}\")\n",
    "\n",
    "# Display model performance\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "metrics_df = pd.DataFrame(pipeline.metrics).T\n",
    "metrics_df = metrics_df.sort_values('rmse')\n",
    "print(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae1203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# MAE comparison\n",
    "mae_values = [pipeline.metrics[model].get('mae', 0) for model in pipeline.metrics.keys()]\n",
    "model_names = list(pipeline.metrics.keys())\n",
    "axes[0].bar(model_names, mae_values, alpha=0.7)\n",
    "axes[0].set_title('Mean Absolute Error (MAE)')\n",
    "axes[0].set_ylabel('MAE')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE comparison\n",
    "rmse_values = [pipeline.metrics[model].get('rmse', 0) for model in pipeline.metrics.keys()]\n",
    "axes[1].bar(model_names, rmse_values, alpha=0.7, color='orange')\n",
    "axes[1].set_title('Root Mean Square Error (RMSE)')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R² comparison\n",
    "r2_values = [pipeline.metrics[model].get('r2', 0) for model in pipeline.metrics.keys()]\n",
    "axes[2].bar(model_names, r2_values, alpha=0.7, color='green')\n",
    "axes[2].set_title('R-squared (R²)')\n",
    "axes[2].set_ylabel('R²')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from best model\n",
    "best_model = min(pipeline.metrics.items(), key=lambda x: x[1].get('rmse', float('inf')))[0]\n",
    "importance = pipeline.get_feature_importance(best_model)\n",
    "\n",
    "if importance:\n",
    "    # Plot top 15 most important features\n",
    "    sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "    features, importances = zip(*sorted_features)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(features)), importances, alpha=0.7)\n",
    "    plt.yticks(range(len(features)), [f.replace('_', ' ') for f in features])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Feature Importance - {best_model.title()}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Features ({best_model}):\")\n",
    "    for feature, imp in sorted_features[:10]:\n",
    "        print(f\"{feature}: {imp:.4f}\")\n",
    "else:\n",
    "    print(\"Feature importance not available for the best model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134564f1",
   "metadata": {},
   "source": [
    "## 6. Predictions and Visualization {#predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cd47c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "y_test = pipeline.predictions['y_test']\n",
    "predictions_to_plot = {}\n",
    "\n",
    "# Get predictions for available models\n",
    "for model_name, pred in pipeline.predictions.items():\n",
    "    if model_name != 'y_test' and isinstance(pred, np.ndarray) and len(pred) == len(y_test):\n",
    "        predictions_to_plot[model_name] = pred\n",
    "\n",
    "# Plot first week of predictions\n",
    "week_length = min(168, len(y_test))  # 1 week or available data\n",
    "y_test_week = y_test.iloc[:week_length]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(y_test_week.index, y_test_week.values, label='Actual', linewidth=3, alpha=0.8)\n",
    "\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(predictions_to_plot)))\n",
    "for i, (model_name, pred) in enumerate(predictions_to_plot.items()):\n",
    "    plt.plot(y_test_week.index, pred[:week_length], \n",
    "             label=model_name.replace('_', ' ').title(), \n",
    "             color=colors[i], linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.title('Energy Consumption: Predictions vs Actual (First Week of Test Data)', fontsize=16)\n",
    "plt.xlabel('Time', fontsize=12)\n",
    "plt.ylabel('Energy Consumption (kWh)', fontsize=12)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01884a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for the best model\n",
    "if predictions_to_plot:\n",
    "    best_pred = predictions_to_plot[best_model]\n",
    "    residuals = y_test.values - best_pred\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Residuals vs Predicted\n",
    "    axes[0, 0].scatter(best_pred, residuals, alpha=0.6)\n",
    "    axes[0, 0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Predicted Values')\n",
    "    axes[0, 0].set_ylabel('Residuals')\n",
    "    axes[0, 0].set_title('Residuals vs Predicted')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram of residuals\n",
    "    axes[0, 1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Residuals')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Distribution of Residuals')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals over time\n",
    "    axes[1, 0].plot(y_test.index, residuals, alpha=0.7)\n",
    "    axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Time')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].set_title('Residuals Over Time')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-Q plot\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title('Q-Q Plot')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Residual Analysis - {best_model.title()}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Residual Statistics for {best_model}:\")\n",
    "    print(f\"Mean: {np.mean(residuals):.4f}\")\n",
    "    print(f\"Std: {np.std(residuals):.4f}\")\n",
    "    print(f\"Min: {np.min(residuals):.4f}\")\n",
    "    print(f\"Max: {np.max(residuals):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb81037",
   "metadata": {},
   "source": [
    "## 7. API Usage Examples {#api-usage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e789da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the trained model for new predictions\n",
    "# This simulates what the API does internally\n",
    "\n",
    "# Get recent consumption data (last 24 hours)\n",
    "recent_data = engineered_data.tail(24)\n",
    "recent_consumption = recent_data['kwh'].tolist()\n",
    "\n",
    "# Simulate weather forecast data\n",
    "weather_forecast = {\n",
    "    'temperature_c': 22.5,\n",
    "    'humidity_percent': 65.0,\n",
    "    'wind_speed_kmh': 12.0,\n",
    "    'precipitation_mm': 0.0,\n",
    "    'cloud_cover_percent': 40.0,\n",
    "    'solar_irradiance_wm2': 450.0\n",
    "}\n",
    "\n",
    "print(\"API Request Example:\")\n",
    "api_request = {\n",
    "    \"recent_consumption\": recent_consumption,\n",
    "    \"weather_forecast\": weather_forecast,\n",
    "    \"forecast_hours\": 24\n",
    "}\n",
    "\n",
    "print(f\"Recent consumption (last 5 values): {recent_consumption[-5:]}\")\n",
    "print(f\"Weather forecast: {weather_forecast}\")\n",
    "print(f\"Forecast hours requested: {api_request['forecast_hours']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53790324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate making predictions with the trained model\n",
    "# Use the best model for prediction\n",
    "if best_model in pipeline.models:\n",
    "    model = pipeline.models[best_model]\n",
    "    \n",
    "    # Use the last available features for prediction\n",
    "    last_features = engineered_data.iloc[-1:].copy()\n",
    "    \n",
    "    # Remove target column\n",
    "    feature_cols = [col for col in last_features.columns if col != 'kwh']\n",
    "    X_pred = last_features[feature_cols]\n",
    "    \n",
    "    # Fill any NaN values\n",
    "    X_pred = X_pred.fillna(0)\n",
    "    \n",
    "    # Make prediction\n",
    "    if hasattr(model, 'predict'):\n",
    "        try:\n",
    "            prediction = model.predict(X_pred)[0]\n",
    "            print(f\"Next hour prediction using {best_model}: {prediction:.3f} kWh\")\n",
    "            \n",
    "            # Compare with actual next value (if available)\n",
    "            last_actual = recent_consumption[-1]\n",
    "            print(f\"Last actual consumption: {last_actual:.3f} kWh\")\n",
    "            print(f\"Prediction difference: {abs(prediction - last_actual):.3f} kWh\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error making prediction: {e}\")\n",
    "    else:\n",
    "        print(f\"Model {best_model} doesn't support direct prediction\")\n",
    "else:\n",
    "    print(\"Best model not found in trained models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ede913",
   "metadata": {},
   "source": [
    "## 8. Results Analysis {#analysis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive results summary\n",
    "print(\"=\"*60)\n",
    "print(\"ENERGY CONSUMPTION PREDICTOR - RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"  Total records: {len(engineered_data)}\")\n",
    "print(f\"  Date range: {engineered_data.index.min()} to {engineered_data.index.max()}\")\n",
    "print(f\"  Duration: {(engineered_data.index.max() - engineered_data.index.min()).days} days\")\n",
    "print(f\"  Features engineered: {len(feature_engineer.get_feature_names())}\")\n",
    "\n",
    "print(f\"\\nConsumption Statistics:\")\n",
    "print(f\"  Average: {engineered_data['kwh'].mean():.3f} kWh\")\n",
    "print(f\"  Median: {engineered_data['kwh'].median():.3f} kWh\")\n",
    "print(f\"  Std Dev: {engineered_data['kwh'].std():.3f} kWh\")\n",
    "print(f\"  Min: {engineered_data['kwh'].min():.3f} kWh\")\n",
    "print(f\"  Max: {engineered_data['kwh'].max():.3f} kWh\")\n",
    "\n",
    "print(f\"\\nModel Performance (sorted by RMSE):\")\n",
    "sorted_models = sorted(pipeline.metrics.items(), key=lambda x: x[1].get('rmse', float('inf')))\n",
    "for i, (model_name, metrics) in enumerate(sorted_models, 1):\n",
    "    mae = metrics.get('mae', 0)\n",
    "    rmse = metrics.get('rmse', 0)\n",
    "    mape = metrics.get('mape', 0)\n",
    "    r2 = metrics.get('r2', 0)\n",
    "    print(f\"  {i}. {model_name.ljust(15)} MAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.2f}%, R²: {r2:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "best_metrics = pipeline.metrics[best_model]\n",
    "print(f\"  Achieved {best_metrics.get('rmse', 0):.4f} RMSE\")\n",
    "print(f\"  Mean Absolute Percentage Error: {best_metrics.get('mape', 0):.2f}%\")\n",
    "print(f\"  R-squared Score: {best_metrics.get('r2', 0):.4f}\")\n",
    "\n",
    "if importance:\n",
    "    print(f\"\\nTop 5 Most Important Features:\")\n",
    "    for feature, imp in sorted(importance.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"  - {feature}: {imp:.4f}\")\n",
    "\n",
    "print(f\"\\nProject Status: ✅ COMPLETED SUCCESSFULLY\")\n",
    "print(f\"Models trained and ready for deployment!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80042e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated the complete Energy Consumption Predictor workflow:\n",
    "\n",
    "1. **Data Loading**: Successfully loaded electricity consumption and weather data\n",
    "2. **Preprocessing**: Cleaned data, handled missing values, and removed outliers\n",
    "3. **Feature Engineering**: Created comprehensive time-based, lag, and statistical features\n",
    "4. **Model Training**: Trained multiple models including XGBoost, LightGBM, and Random Forest\n",
    "5. **Evaluation**: Compared model performance using MAE, RMSE, MAPE, and R² metrics\n",
    "6. **Visualization**: Created comprehensive plots for analysis\n",
    "\n",
    "### Key Insights:\n",
    "- The best performing model achieved low prediction errors\n",
    "- Lag features and rolling statistics are among the most important predictors\n",
    "- Time-based features help capture daily and weekly patterns\n",
    "- Weather features provide additional predictive power\n",
    "\n",
    "### Next Steps:\n",
    "1. Deploy the API server: `python main.py serve`\n",
    "2. Test the API endpoints with real data\n",
    "3. Set up automated model retraining\n",
    "4. Monitor model performance in production\n",
    "5. Collect more data to improve accuracy\n",
    "\n",
    "The models are now ready for production use and can help optimize energy consumption and reduce electricity costs!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
